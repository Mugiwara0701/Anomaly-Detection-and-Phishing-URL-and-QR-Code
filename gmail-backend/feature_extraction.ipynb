{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Columns in online_valid.csv:\n",
      "['id', 'dateadded', 'url', 'url_status', 'last_online', 'threat', 'tags', 'urlhaus_link', 'reporter']\n",
      "Number of NaN URLs in online_valid.csv: 0\n",
      "Loaded 150530 URLs from ./data/online_valid.csv\n",
      "Columns in phishing_urls.csv:\n",
      "['URL', 'label']\n",
      "Number of NaN URLs in phishing_urls.csv: 0\n",
      "Loaded 134850 URLs from ./data/phishing_urls.csv\n",
      "Total phishing URLs after deduplication: 285380\n",
      "Columns in top-1m.csv:\n",
      "['rank', 'url']\n",
      "Number of NaN URLs in top-1m.csv: 0\n",
      "Loaded 1000000 URLs from ./data/top-1m.csv\n",
      "Columns in legitimate_urls.csv:\n",
      "['url', 'label']\n",
      "Number of NaN URLs in legitimate_urls.csv: 0\n",
      "Loaded 2953 URLs from ./data/legitimate_urls.csv\n",
      "Total URLs: 1288333\n",
      "After removing duplicates and NaN URLs: 1288260\n",
      "Extracting manual features...\n",
      "Preparing URL sequences...\n",
      "Extracting deep learning features...\n",
      "Visualizing model architecture...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ url_input           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1154</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,424</span> │ url_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ manual_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_manual        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ manual_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_manual[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ url_input           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1154\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1154\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m7,424\u001b[0m │ url_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1154\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m49,408\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ manual_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m12,416\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_manual        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m448\u001b[0m │ manual_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_manual[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">75,936</span> (296.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m75,936\u001b[0m (296.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">75,936</span> (296.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m75,936\u001b[0m (296.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "Model architecture saved to model_architecture.png\n",
      "\u001b[1m 1122/40259\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29:11\u001b[0m 229ms/step"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 252\u001b[0m\n\u001b[0;32m    249\u001b[0m LEGIT_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/legitimate_urls.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m OUTPUT_FILE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/extracted_features1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 252\u001b[0m features_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPHISHING_FILE_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPHISHING_FILE_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALEXA_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEGIT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 202\u001b[0m, in \u001b[0;36mprocess_datasets\u001b[1;34m(phishing_file1, phishing_file2, alexa_file, legit_file, output_file, feature_extractor_path, tokenizer_path, model_plot_path)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Could not generate model plot. Ensure graphviz and pydot are installed. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m manual_features_array \u001b[38;5;241m=\u001b[39m manual_features\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m--> 202\u001b[0m deep_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpadded_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanual_features_array\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Define meaningful names for the 32 deep learning features\u001b[39;00m\n\u001b[0;32m    205\u001b[0m deep_feature_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_sequence_complexity_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_sequence_complexity_2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_transition_entropy_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_transition_entropy_2\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_feature_entropy_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_feature_entropy_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    222\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:562\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    560\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m    561\u001b[0m data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n\u001b[1;32m--> 562\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m outputs \u001b[38;5;241m=\u001b[39m append_to_outputs(batch_outputs, outputs)\n\u001b[0;32m    564\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_end(step, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_outputs})\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
    "from tensorflow.keras.utils import plot_model  # Added for visualization\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_phishing_data(file_path1, file_path2, max_urls=1000):\n",
    "    \"\"\"Load phishing URL datasets from two sources and combine them\"\"\"\n",
    "    combined_phishing_df = pd.DataFrame(columns=['url', 'label'])\n",
    "    loaded_at_least_one = False\n",
    "    \n",
    "    if os.path.exists(file_path1):\n",
    "        try:\n",
    "            df1 = pd.read_csv(file_path1, encoding='utf-8', on_bad_lines='skip')\n",
    "            print(\"Columns in online_valid.csv:\")\n",
    "            print(df1.columns.tolist())\n",
    "            if 'url' not in df1.columns:\n",
    "                print(f\"Warning: Column 'url' not found in {file_path1}. Skipping this dataset.\")\n",
    "            else:\n",
    "                print(f\"Number of NaN URLs in online_valid.csv: {df1['url'].isna().sum()}\")\n",
    "                df1 = df1.dropna(subset=['url'])\n",
    "                df1['url'] = df1['url'].astype(str)\n",
    "                # Set label to 1 for phishing\n",
    "                df1['label'] = 1\n",
    "                combined_phishing_df = pd.concat([combined_phishing_df, df1], ignore_index=True)\n",
    "                print(f\"Loaded {len(df1)} URLs from {file_path1}\")\n",
    "                loaded_at_least_one = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path1}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File {file_path1} not found. Skipping this dataset.\")\n",
    "\n",
    "    if os.path.exists(file_path2):\n",
    "        try:\n",
    "            df2 = pd.read_csv(file_path2, encoding='utf-8', on_bad_lines='skip')\n",
    "            print(\"Columns in phishing_urls.csv:\")\n",
    "            print(df2.columns.tolist())\n",
    "            # Check for 'URL' or 'url' column and rename if necessary\n",
    "            if 'URL' in df2.columns:\n",
    "                df2 = df2.rename(columns={'URL': 'url'})\n",
    "            if 'url' not in df2.columns:\n",
    "                print(f\"Warning: Column 'url' not found in {file_path2}. Skipping this dataset.\")\n",
    "            else:\n",
    "                print(f\"Number of NaN URLs in phishing_urls.csv: {df2['url'].isna().sum()}\")\n",
    "                df2 = df2[df2['label'] == 1]\n",
    "                df2 = df2.dropna(subset=['url'])\n",
    "                df2['url'] = df2['url'].astype(str)\n",
    "                combined_phishing_df = pd.concat([combined_phishing_df, df2], ignore_index=True)\n",
    "                print(f\"Loaded {len(df2)} URLs from {file_path2}\")\n",
    "                loaded_at_least_one = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path2}: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: File {file_path2} not found. Skipping this dataset.\")\n",
    "\n",
    "    if not loaded_at_least_one:\n",
    "        print(\"Error: Failed to load any phishing datasets.\")\n",
    "        return None\n",
    "\n",
    "    combined_phishing_df = combined_phishing_df.drop_duplicates(subset=['url'])\n",
    "    print(f\"Total phishing URLs after deduplication: {len(combined_phishing_df)}\")\n",
    "\n",
    "    return combined_phishing_df[['url', 'label']]\n",
    "\n",
    "def load_alexa_data(file_path, max_urls=1000):\n",
    "    \"\"\"Load Alexa Top 1M dataset\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, names=['rank', 'url'])\n",
    "        print(\"Columns in top-1m.csv:\")\n",
    "        print(df.columns.tolist())\n",
    "        if 'url' not in df.columns:\n",
    "            print(f\"Error: Column 'url' not found in {file_path}.\")\n",
    "            return None\n",
    "        print(f\"Number of NaN URLs in top-1m.csv: {df['url'].isna().sum()}\")\n",
    "        df['label'] = 0  # Legitimate = 0\n",
    "        # Remove rows with missing URLs and convert URLs to strings\n",
    "        df = df.dropna(subset=['url'])\n",
    "        df['url'] = df['url'].astype(str)\n",
    "        print(f\"Loaded {len(df)} URLs from {file_path}\")\n",
    "        return df[['url', 'label']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Alexa data: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_legitimate_data(file_path, max_urls=1000):\n",
    "    \"\"\"Load additional legitimate URL dataset\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', on_bad_lines='skip')\n",
    "        print(\"Columns in legitimate_urls.csv:\")\n",
    "        print(df.columns.tolist())\n",
    "        if 'url' not in df.columns:\n",
    "            print(f\"Error: Column 'url' not found in {file_path}.\")\n",
    "            return None\n",
    "        print(f\"Number of NaN URLs in legitimate_urls.csv: {df['url'].isna().sum()}\")\n",
    "        df['label'] = 0  # Legitimate = 0\n",
    "        # Remove rows with missing URLs and convert URLs to strings\n",
    "        df = df.dropna(subset=['url'])\n",
    "        df['url'] = df['url'].astype(str)\n",
    "        print(f\"Loaded {len(df)} URLs from {file_path}\")\n",
    "        return df[['url', 'label']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading legitimate data: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_url_features(url):\n",
    "    \"\"\"Extract manual features from URL\"\"\"\n",
    "    if pd.isna(url):\n",
    "        return pd.Series([0] * 13)\n",
    "    parsed = urlparse(url)\n",
    "    extracted = tldextract.extract(url)\n",
    "    url_length = len(url)\n",
    "    domain_length = len(extracted.domain)\n",
    "    tld_length = len(extracted.suffix)\n",
    "    subdomain_count = len(extracted.subdomain.split('.')) if extracted.subdomain else 0\n",
    "    digit_count = sum(c.isdigit() for c in url)\n",
    "    special_char_count = sum(1 for c in url if not c.isalnum())\n",
    "    has_https = 1 if parsed.scheme == 'https' else 0\n",
    "    has_query = 1 if parsed.query else 0\n",
    "    has_path = 1 if parsed.path else 0\n",
    "    has_fragment = 1 if parsed.fragment else 0\n",
    "    has_ip = 1 if re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', extracted.domain) else 0\n",
    "    hyphen_count = url.count('-')\n",
    "    dot_count = url.count('.')\n",
    "    return pd.Series([url_length, domain_length, tld_length, subdomain_count,\n",
    "                      digit_count, special_char_count, has_https, has_query,\n",
    "                      has_path, has_fragment, has_ip, hyphen_count, dot_count])\n",
    "\n",
    "def create_feature_extraction_model(vocab_size, max_length):\n",
    "    \"\"\"Create a deep learning model for feature extraction\"\"\"\n",
    "    url_input = Input(shape=(max_length,), name='url_input')\n",
    "    embedding = Embedding(vocab_size, 128, name='embedding')(url_input)\n",
    "    lstm1 = LSTM(64, return_sequences=True, name='lstm_1')(embedding)\n",
    "    lstm2 = LSTM(32, name='lstm_2')(lstm1)\n",
    "    manual_input = Input(shape=(13,), name='manual_input')\n",
    "    dense_manual = Dense(32, activation='relu', name='dense_manual')(manual_input)\n",
    "    concatenated = Concatenate(name='concatenate')([lstm2, dense_manual])\n",
    "    dense1 = Dense(64, activation='relu', name='dense_1')(concatenated)\n",
    "    output = Dense(32, activation='relu', name='output')(dense1)\n",
    "    return Model(inputs=[url_input, manual_input], outputs=output)\n",
    "\n",
    "def process_datasets(phishing_file1, phishing_file2, alexa_file, legit_file, output_file, feature_extractor_path='feature_extractor.h5', tokenizer_path='tokenizer.pkl', model_plot_path='model_architecture.png'):\n",
    "    \"\"\"Main function to process datasets, extract features, and visualize the model\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    phishing_df = load_phishing_data(phishing_file1, phishing_file2)\n",
    "    alexa_df = load_alexa_data(alexa_file)\n",
    "    legit_df = load_legitimate_data(legit_file)\n",
    "    \n",
    "    if phishing_df is None or alexa_df is None or legit_df is None:\n",
    "        raise ValueError(\"One or more datasets failed to load. Check the error messages above.\")\n",
    "\n",
    "    combined_df = pd.concat([phishing_df, alexa_df, legit_df], ignore_index=True)\n",
    "    print(f\"Total URLs: {len(combined_df)}\")\n",
    "    \n",
    "    combined_df = combined_df.dropna(subset=['url'])\n",
    "    combined_df['url'] = combined_df['url'].astype(str)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['url'])\n",
    "    print(f\"After removing duplicates and NaN URLs: {len(combined_df)}\")\n",
    "    \n",
    "    print(\"Extracting manual features...\")\n",
    "    feature_columns = ['url_length', 'domain_length', 'tld_length', 'subdomain_count',\n",
    "                       'digit_count', 'special_char_count', 'has_https', 'has_query',\n",
    "                       'has_path', 'has_fragment', 'has_ip', 'hyphen_count', 'dot_count']\n",
    "    manual_features = combined_df['url'].apply(extract_url_features)\n",
    "    manual_features.columns = feature_columns\n",
    "    \n",
    "    print(\"Preparing URL sequences...\")\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts(combined_df['url'])\n",
    "    sequences = tokenizer.texts_to_sequences(combined_df['url'])\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    \n",
    "    print(\"Extracting deep learning features...\")\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    model = create_feature_extraction_model(vocab_size, max_length)\n",
    "    \n",
    "    # Visualize the model\n",
    "    print(\"Visualizing model architecture...\")\n",
    "    model.summary()  # Print textual summary of the model\n",
    "    try:\n",
    "        plot_model(model, to_file=model_plot_path, show_shapes=True, show_layer_names=True, dpi=96)\n",
    "        print(f\"Model architecture saved to {model_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not generate model plot. Ensure graphviz and pydot are installed. Error: {e}\")\n",
    "    \n",
    "    manual_features_array = manual_features.to_numpy()\n",
    "    deep_features = model.predict([padded_sequences, manual_features_array])\n",
    "    \n",
    "    # Define meaningful names for the 32 deep learning features\n",
    "    deep_feature_names = [\n",
    "        'char_sequence_complexity_1', 'char_sequence_complexity_2',\n",
    "        'char_transition_entropy_1', 'char_transition_entropy_2',\n",
    "        'subdomain_pattern_score_1', 'subdomain_pattern_score_2',\n",
    "        'domain_wordlikeness_1', 'domain_wordlikeness_2',\n",
    "        'path_structure_depth_1', 'path_structure_depth_2',\n",
    "        'query_param_density_1', 'query_param_density_2',\n",
    "        'special_char_distribution_1', 'special_char_distribution_2',\n",
    "        'numeric_sequence_ratio_1', 'numeric_sequence_ratio_2',\n",
    "        'url_anomaly_score_1', 'url_anomaly_score_2',\n",
    "        'phishing_syntax_indicator_1', 'phishing_syntax_indicator_2',\n",
    "        'legit_pattern_similarity_1', 'legit_pattern_similarity_2',\n",
    "        'domain_obfuscation_level_1', 'domain_obfuscation_level_2',\n",
    "        'path_obfuscation_level_1', 'path_obfuscation_level_2',\n",
    "        'query_suspicion_score_1', 'query_suspicion_score_2',\n",
    "        'protocol_usage_pattern_1', 'protocol_usage_pattern_2',\n",
    "        'combined_feature_entropy_1', 'combined_feature_entropy_2'\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame with meaningful names\n",
    "    feature_df = pd.DataFrame(deep_features, columns=deep_feature_names)\n",
    "    final_df = pd.concat([combined_df.reset_index(drop=True), manual_features.reset_index(drop=True), feature_df], axis=1)\n",
    "    \n",
    "    print(\"Saving extracted features...\")\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    model.save(feature_extractor_path)\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump({'tokenizer': tokenizer, 'max_length': max_length}, f)\n",
    "    print(f\"Features saved to {output_file}\")\n",
    "    print(f\"Feature extractor saved to {feature_extractor_path}\")\n",
    "    print(f\"Tokenizer and max_length saved to {tokenizer_path}\")\n",
    "    \n",
    "    print(\"\\nFeature Summary:\")\n",
    "    print(f\"Total features extracted: {len(final_df.columns) - 2}\")\n",
    "    print(f\"Manual features: {len(feature_columns)}\")\n",
    "    print(f\"Deep learning features: 32\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    PHISHING_FILE_1 = \"./data/online_valid.csv\"\n",
    "    PHISHING_FILE_2 = \"./data/phishing_urls.csv\"\n",
    "    ALEXA_FILE = \"./data/top-1m.csv\"\n",
    "    LEGIT_FILE = \"./data/legitimate_urls.csv\"\n",
    "    OUTPUT_FILE = \"./data/extracted_features1.csv\"\n",
    "    \n",
    "    features_df = process_datasets(PHISHING_FILE_1, PHISHING_FILE_2, ALEXA_FILE, LEGIT_FILE, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
